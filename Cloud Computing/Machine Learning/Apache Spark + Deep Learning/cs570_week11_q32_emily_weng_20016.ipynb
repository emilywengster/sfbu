{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chSBAdvh2ofS",
        "outputId": "37d7a400-25af-4025-a913-d7a21db2e0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=3a6fd0fa37e4ba3ce45cb0b66cd140e9fb6b21e06ac04ca5b17e12f3403d7f3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "rBdvBiOV3p97"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "6x6YoeC-30CI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init('/content/spark-3.1.2-bin-hadoop2.7')\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "mhpKEOW532_8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "curl -O http://download.tensorflow.org/example_images/flower_photos.tgz\n",
        "tar xzf flower_photos.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ySCTpwk3Rwd",
        "outputId": "08729d94-3673-41b9-868a-23acf1cc5dab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  218M    0 1022k    0     0   916k      0  0:04:03  0:00:01  0:04:02  916k\r 11  218M   11 25.9M    0     0  12.1M      0  0:00:17  0:00:02  0:00:15 12.1M\r 25  218M   25 55.5M    0     0  17.8M      0  0:00:12  0:00:03  0:00:09 17.8M\r 39  218M   39 85.5M    0     0  20.7M      0  0:00:10  0:00:04  0:00:06 20.7M\r 53  218M   53  116M    0     0  22.7M      0  0:00:09  0:00:05  0:00:04 23.8M\r 66  218M   66  146M    0     0  23.9M      0  0:00:09  0:00:06  0:00:03 29.0M\r 77  218M   77  169M    0     0  23.8M      0  0:00:09  0:00:07  0:00:02 28.8M\r 91  218M   91  200M    0     0  24.6M      0  0:00:08  0:00:08 --:--:-- 28.9M\r100  218M  100  218M    0     0  25.0M      0  0:00:08  0:00:08 --:--:-- 28.8M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3TabGC34rtY",
        "outputId": "c6d65733-7d43-4688-bf74-ae0b699501f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "directory = 'flower_photos'\n",
        "for filename in os.listdir(directory):\n",
        "    print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "352EKckw4zAb",
        "outputId": "3d34ef3b-173e-499b-f0fb-4a6f84f9d8b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daisy\n",
            "sunflowers\n",
            "tulips\n",
            "roses\n",
            "LICENSE.txt\n",
            "dandelion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_dir = '/content/flower_photos'\n",
        "os.makedirs(img_dir + \"/tulips\", exist_ok=True)\n",
        "os.makedirs(img_dir + \"/daisy\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RwocwwfW74bf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = '/content/flower_photos'\n",
        "img_dir = 'content/photos'  # Make sure this is defined correctly\n",
        "\n",
        "def copy_tree(src, dst):\n",
        "    try:\n",
        "        if not os.path.exists(dst):\n",
        "            os.makedirs(dst)\n",
        "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "        print(f\"Successfully copied from {src} to {dst}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying from {src} to {dst}: {e}\")\n",
        "\n",
        "copy_tree(os.path.join(source_dir, 'tulips'), os.path.join(img_dir, 'tulips'))\n",
        "copy_tree(os.path.join(source_dir, 'daisy'), os.path.join(img_dir, 'daisy'))\n",
        "\n",
        "try:\n",
        "    shutil.copy(os.path.join(source_dir, 'LICENSE.txt'), img_dir)\n",
        "    print(\"Successfully copied LICENSE.txt\")\n",
        "except Exception as e:\n",
        "    print(f\"Error copying LICENSE.txt: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGh4WrK877RC",
        "outputId": "ba7304c9-385f-4ae1-918a-cdaaa7adb089"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully copied from /content/flower_photos/tulips to content/photos/tulips\n",
            "Successfully copied from /content/flower_photos/daisy to content/photos/daisy\n",
            "Successfully copied LICENSE.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = '/content/content/photos'  # Ensure this is correctly defined\n",
        "sample_img_dir = os.path.join(img_dir, 'sample')\n",
        "\n",
        "# Create the sample image directory\n",
        "os.makedirs(sample_img_dir, exist_ok=True)\n",
        "\n",
        "# List files in 'tulips' and 'daisy' directories\n",
        "def list_files(directory, num_files):\n",
        "    return sorted([os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])[:num_files]\n",
        "\n",
        "tulips_files = list_files(os.path.join(img_dir, 'tulips'), 1)\n",
        "daisy_files = list_files(os.path.join(img_dir, 'daisy'), 2)\n",
        "files = tulips_files + daisy_files\n",
        "\n",
        "# Copy selected files to 'sample_img_dir'\n",
        "for file_path in files:\n",
        "    try:\n",
        "        shutil.copy(file_path, sample_img_dir)\n",
        "        print(f\"Copied {file_path} to {sample_img_dir}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error copying {file_path}: {e}\")\n",
        "\n",
        "# List and display contents of the sample image directory\n",
        "sample_img_dir_contents = os.listdir(sample_img_dir)\n",
        "print(\"Contents of sample_img_dir:\")\n",
        "for item in sample_img_dir_contents:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J164ZBEL8vt5",
        "outputId": "b9af75a4-2d8e-4ed1-ce8d-73661c599cbd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied /content/content/photos/tulips/100930342_92e8746431_n.jpg to /content/content/photos/sample\n",
            "Copied /content/content/photos/daisy/100080576_f52e8ee070_n.jpg to /content/content/photos/sample\n",
            "Copied /content/content/photos/daisy/10140303196_b88d3d6cec.jpg to /content/content/photos/sample\n",
            "Contents of sample_img_dir:\n",
            "100930342_92e8746431_n.jpg\n",
            "10140303196_b88d3d6cec.jpg\n",
            "100080576_f52e8ee070_n.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define the directory containing the images\n",
        "sample_img_dir = '/content/photos/sample'  # Update this path as needed\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, [224, 224])  # Resize to a standard size\n",
        "    img = img / 255.0  # Normalize to [0, 1] range\n",
        "    return img\n",
        "\n",
        "# Create a TensorFlow Dataset to read images\n",
        "def load_images_from_directory(directory):\n",
        "    image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if os.path.isfile(os.path.join(directory, fname))]\n",
        "    image_paths_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    image_ds = image_paths_ds.map(lambda path: tf.numpy_function(func=lambda p: load_and_preprocess_image(p.decode('utf-8')), inp=[path], Tout=tf.float32))\n",
        "    return image_ds\n",
        "\n",
        "# Load images\n",
        "image_ds = load_images_from_directory(sample_img_dir)\n",
        "\n",
        "# Display some images\n",
        "def display_images(image_ds, num_images=3):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i, img in enumerate(image_ds.take(num_images)):\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "        plt.imshow(img.numpy())\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display images\n",
        "display_images(image_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-DCKAQg2-Lul",
        "outputId": "f67a5db8-1cf4-4d16-ebb0-b671e6db2d42"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define directories\n",
        "tulips_dir = '/content/content/photos/tulips'\n",
        "daisy_dir = '/content/content/photos/daisy'\n",
        "\n",
        "# Function to load images from a directory and assign labels\n",
        "def load_images_and_labels(directory, label):\n",
        "    image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if os.path.isfile(os.path.join(directory, fname))]\n",
        "    images = []\n",
        "    labels = []\n",
        "    for path in image_paths:\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3)\n",
        "        img = tf.image.resize(img, [224, 224])  # Resize to a standard size\n",
        "        img = img / 255.0  # Normalize to [0, 1] range\n",
        "        images.append(img.numpy())\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Load images and labels\n",
        "tulips_images, tulips_labels = load_images_and_labels(tulips_dir, 1)\n",
        "daisy_images, daisy_labels = load_images_and_labels(daisy_dir, 0)\n",
        "\n",
        "# Combine images and labels into a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'image': tulips_images + daisy_images,\n",
        "    'label': tulips_labels + daisy_labels\n",
        "})\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'])\n",
        "\n",
        "# Convert DataFrames to TensorFlow Datasets\n",
        "def df_to_tf_dataset(df, batch_size=32):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(df['image']), list(df['label'])))\n",
        "    dataset = dataset.shuffle(buffer_size=len(df))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "train_ds = df_to_tf_dataset(train_df)\n",
        "test_ds = df_to_tf_dataset(test_df)\n",
        "\n",
        "# Optionally: Adjust batch size or other parameters based on needs\n"
      ],
      "metadata": {
        "id": "aD4nZ1iV-aP5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define directories\n",
        "tulips_dir = '/content/content/photos/tulips'\n",
        "daisy_dir = '/content/content/photos/daisy'\n",
        "\n",
        "# Function to load images from a directory and assign labels\n",
        "def load_images_and_labels(directory, label):\n",
        "    image_paths = [os.path.join(directory, fname) for fname in os.listdir(directory) if os.path.isfile(os.path.join(directory, fname))]\n",
        "    images = []\n",
        "    labels = []\n",
        "    for path in image_paths:\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3)\n",
        "        img = tf.image.resize(img, [299, 299])  # Resize to a standard size\n",
        "        img = img / 255.0  # Normalize to [0, 1] range\n",
        "        images.append(img.numpy())\n",
        "        labels.append(label)\n",
        "    return images, labels\n",
        "\n",
        "# Extract features from images\n",
        "def extract_features(images):\n",
        "    # Load the InceptionV3 model pre-trained on ImageNet\n",
        "    model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', pooling='avg')\n",
        "\n",
        "    # Preprocess images\n",
        "    images = [tf.image.resize(img, [299, 299]) for img in images]\n",
        "    images = np.array([tf.keras.applications.inception_v3.preprocess_input(img) for img in images])\n",
        "\n",
        "    # Extract features\n",
        "    features = model.predict(images)\n",
        "    return features\n",
        "\n",
        "# Load and preprocess data\n",
        "tulips_images, tulips_labels = load_images_and_labels(tulips_dir, 1)\n",
        "daisy_images, daisy_labels = load_images_and_labels(daisy_dir, 0)\n",
        "\n",
        "# Combine images and labels into a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'image': tulips_images + daisy_images,\n",
        "    'label': tulips_labels + daisy_labels\n",
        "})\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'])\n",
        "\n",
        "# Extract features from training and testing data\n",
        "X_train = extract_features(train_df['image'].tolist())\n",
        "y_train = train_df['label'].values\n",
        "X_test = extract_features(test_df['image'].tolist())\n",
        "y_test = test_df['label'].values\n",
        "\n",
        "# Create and train the Logistic Regression model using the 'saga' solver\n",
        "lr = LogisticRegression(max_iter=20, solver='saga', penalty='elasticnet', l1_ratio=0.3, C=1/0.05)\n",
        "\n",
        "# Train the model\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy = {accuracy:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCDhAY_r-ydA",
        "outputId": "15439ede-65b9-4776-afa4-8f0d1cf08dcb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36/36 [==============================] - 288s 8s/step\n",
            "9/9 [==============================] - 73s 8s/step\n",
            "Test set accuracy = 0.812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy = {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KdnKcO7BW4R",
        "outputId": "5eb0a9ac-0651-4e44-e0df-c7740719ed98"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy = 0.812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df is the DataFrame with 'probability' and 'label' columns\n",
        "# For demonstration purposes, I'll create a sample DataFrame\n",
        "# Replace this with actual DataFrame creation/loading code\n",
        "data = {\n",
        "    'filePath': ['path/to/img1', 'path/to/img2', 'path/to/img3'],\n",
        "    'probability': [np.array([0.1, 0.9]), np.array([0.8, 0.2]), np.array([0.4, 0.6])],\n",
        "    'label': [1, 0, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract the probability for the positive class (index 1)\n",
        "df['p_1'] = df['probability'].apply(lambda v: float(v[1]))\n",
        "\n",
        "# Sort DataFrame by the absolute difference between 'p_1' and 'label'\n",
        "df['abs_diff'] = np.abs(df['p_1'] - df['label'])\n",
        "wrong_df = df.sort_values(by='abs_diff', ascending=False)\n",
        "\n",
        "# Display the top 10 rows\n",
        "print(wrong_df[['filePath', 'p_1', 'label']].head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXDWiWSBDtFC",
        "outputId": "dae741e5-4eb1-49b3-c15f-e193b17c26dc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       filePath  p_1  label\n",
            "2  path/to/img3  0.6      1\n",
            "1  path/to/img2  0.2      0\n",
            "0  path/to/img1  0.9      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet\n",
        "model = InceptionV3(weights='imagenet', include_top=True)\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))  # Resize to match InceptionV3 input size\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_image(img_path):\n",
        "    img_array = load_and_preprocess_image(img_path)\n",
        "    print(f\"Predicting for image: {img_path}\")  # Debug print\n",
        "    predictions = model.predict(img_array)\n",
        "    decoded_predictions = decode_predictions(predictions, top=10)[0]\n",
        "    print(f\"Predictions: {decoded_predictions}\")  # Debug print\n",
        "    pred_labels = [f\"{label}: {prob:.4f}\" for (_, label, prob) in decoded_predictions]\n",
        "    return pred_labels\n",
        "\n",
        "# Load image paths and make predictions\n",
        "image_paths = [os.path.join(sample_img_dir, fname) for fname in os.listdir(sample_img_dir) if os.path.isfile(os.path.join(sample_img_dir, fname))]\n",
        "print(f\"Image paths: {image_paths}\")  # Debug print\n",
        "\n",
        "results = []\n",
        "for img_path in image_paths:\n",
        "    preds = predict_image(img_path)\n",
        "    results.append({\n",
        "        'filePath': img_path,\n",
        "        'predicted_labels': ', '.join(preds)\n",
        "    })\n",
        "\n",
        "# Print results to verify\n",
        "print(f\"Results: {results}\")\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "predictions_df = pd.DataFrame(results)\n",
        "\n",
        "# Print column names and first few rows to verify\n",
        "print(predictions_df.columns)\n",
        "print(predictions_df.head())\n",
        "\n",
        "# Display the results\n",
        "print(predictions_df[['filePath', 'predicted_labels']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "_-HCV28cD1A9",
        "outputId": "b38a7f38-9142-4cf9-f31b-1eb278138b2f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image paths: []\n",
            "Results: []\n",
            "RangeIndex(start=0, stop=0, step=1)\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['filePath', 'predicted_labels'], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-c52bd7bbe6ac>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Display the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'filePath'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predicted_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3898\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3899\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6113\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6115\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6117\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6175\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6176\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6178\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['filePath', 'predicted_labels'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load the InceptionV3 model pre-trained on ImageNet\n",
        "model = InceptionV3(weights='imagenet', include_top=True)\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(299, 299))  # Resize to match InceptionV3 input size\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions and get probabilities for 'daisy'\n",
        "def predict_image(img_path):\n",
        "    img_array = load_and_preprocess_image(img_path)\n",
        "    predictions = model.predict(img_array)\n",
        "    decoded_predictions = decode_predictions(predictions, top=10)[0]\n",
        "    # Get the probability for the 'daisy' class\n",
        "    daisy_prob = next((prob for (_, label, prob) in decoded_predictions if label == 'daisy'), 0)\n",
        "    return daisy_prob\n",
        "\n",
        "# Load image paths\n",
        "sample_img_dir = '/content/content/photos/sample'  # Update with your image directory path\n",
        "image_paths = [os.path.join(sample_img_dir, fname) for fname in os.listdir(sample_img_dir) if os.path.isfile(os.path.join(sample_img_dir, fname))]\n",
        "\n",
        "# Get predictions\n",
        "results = []\n",
        "for img_path in image_paths:\n",
        "    daisy_prob = predict_image(img_path)\n",
        "    results.append({\n",
        "        'filePath': img_path,\n",
        "        'p_daisy': daisy_prob\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "predictions_df = pd.DataFrame(results)\n",
        "\n",
        "# Process probabilities: Calculate (1 - p_daisy)\n",
        "predictions_df['p_daisy'] = 1 - predictions_df['p_daisy']\n",
        "\n",
        "# Display the results\n",
        "print(predictions_df[['filePath', 'p_daisy']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vd48RdyUD2CB",
        "outputId": "8ca92c03-56af-43d3-a2bc-77eb9edc1314"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 1us/step\n",
            "1/1 [==============================] - 0s 405ms/step\n",
            "1/1 [==============================] - 0s 430ms/step\n",
            "                                            filePath   p_daisy\n",
            "0  /content/content/photos/sample/100930342_92e87...  0.861116\n",
            "1  /content/content/photos/sample/10140303196_b88...  0.046790\n",
            "2  /content/content/photos/sample/100080576_f52e8...  0.108185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "import os\n",
        "\n",
        "# Define constants\n",
        "IMAGE_SIZE = (299, 299)\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=IMAGE_SIZE)  # Resize to match model input size\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n",
        "\n",
        "# Function to transform images using TensorFlow\n",
        "def transform_image(img_path):\n",
        "    img_array = load_and_preprocess_image(img_path)\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "\n",
        "    # Create a TensorFlow function to run the model\n",
        "    model_function = tf.function(lambda x: model(x))\n",
        "\n",
        "    # Run the model on the image\n",
        "    transformed_images = model_function(img_array)\n",
        "\n",
        "    return transformed_images\n",
        "\n",
        "# Load image paths\n",
        "sample_img_dir = '/content/content/photos/sample'  # Update with your image directory path\n",
        "image_paths = [os.path.join(sample_img_dir, fname) for fname in os.listdir(sample_img_dir) if os.path.isfile(os.path.join(sample_img_dir, fname))]\n",
        "\n",
        "# Transform images\n",
        "transformed_images = []\n",
        "for img_path in image_paths:\n",
        "    transformed_img = transform_image(img_path)\n",
        "    transformed_images.append({\n",
        "        'filePath': img_path,\n",
        "        'transformed_image': transformed_img.numpy().squeeze()  # Remove batch dimension\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "transformed_df = pd.DataFrame(transformed_images)\n",
        "\n",
        "# Print the DataFrame\n",
        "print(transformed_df[['filePath']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCSr46O2FAJz",
        "outputId": "4c894cf9-30e5-4345-d70b-86ef22f3a3b2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            filePath\n",
            "0  /content/content/photos/sample/100930342_92e87...\n",
            "1  /content/content/photos/sample/10140303196_b88...\n",
            "2  /content/content/photos/sample/100080576_f52e8...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def remove_dir(dir_path):\n",
        "    \"\"\"Remove a directory and its contents.\"\"\"\n",
        "    if os.path.exists(dir_path):\n",
        "        shutil.rmtree(dir_path)\n",
        "        print(f\"Removed directory: {dir_path}\")\n",
        "    else:\n",
        "        print(f\"Directory does not exist: {dir_path}\")\n",
        "\n",
        "# Define your directories\n",
        "img_dir = '/content/content/photos'  # Update with your image directory path\n",
        "dbfs_model_path = '/content/content/model'  # Update with your model path\n",
        "\n",
        "# Remove directories\n",
        "remove_dir(img_dir)\n",
        "remove_dir(dbfs_model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqp08vOGFLVQ",
        "outputId": "280ad5ae-7d2c-4e61-ae2c-a19c4ae2ac0d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed directory: /content/content/photos\n",
            "Directory does not exist: /content/content/model\n"
          ]
        }
      ]
    }
  ]
}